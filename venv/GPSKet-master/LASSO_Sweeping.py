import numpy as np
import netket as nk

import GPSKet

from GPSKet.models import qGPS

import jax.numpy as jnp

from GPSKet.supervised.supervised_qgps import QGPSLogSpaceFit

import jax


import optax

import GPSKet.operator.hamiltonian.J1J2 as j1j2

import sklearn
from sklearn.linear_model import Lasso

import matplotlib.pyplot as plt

from functools import partial


def initialise_system(L,M):
    """Initialises the system Hamiltonian and generates a variational quantum state using the qGPS model."""
    ha = j1j2.get_J1_J2_Hamiltonian(Lx=L, J2=0.0, sign_rule=True, on_the_fly_en=True)
    hi = ha.hilbert

    model = qGPS(hi, M, init_fun=GPSKet.nn.initializers.normal(1.0e-3), dtype=float)

    sa = nk.sampler.MetropolisExchange(hi, graph=ha.graph, n_chains_per_rank=1, d_max=L)
    vs = nk.vqs.MCState(sa, model, n_samples=10*M*L, chunk_size=1, seed=1)

    return vs, ha

def generate_test_data(ha):
    """Generate the exact ground state energy, amplitudes, log amplitudes and the basis state configurations for the inputted Hamiltonian."""
    e, state = nk.exact.lanczos_ed(ha, compute_eigenvectors=True, k=1)
    configs = jnp.array(ha.hilbert.states_to_local_indices(ha.hilbert.all_states()))
    amps = jnp.array(state.flatten())
    log_amps = jnp.log(amps * jnp.sign(amps[0]))
    return e, configs, amps, log_amps

def lossfun(
    log_amps: list,
    configs: list,
    epsilon, 
    indices: list, 
    vs, 
    weight_psi_sq
    ):
    """Least Square Error loss function on the log amplitudes."""
    indices = jnp.atleast_1d(
        indices
    )  # Another reformatting step to make sure the indices are in the correct format.
    estimated_amps = jnp.exp(vs._apply_fun({"params": {"epsilon": epsilon}}, configs))
    estimated_amps = estimated_amps/jnp.linalg.norm(estimated_amps)
    sampled_log_amps = jnp.log(estimated_amps)[indices]
    # The log amplitudes, of the state parameterised by epsilon and the qGPS model, of the configurations refferred to by the indices passed into lossfun.
    # Weightings are an additional step that weight the Loss Function to be affected more by the error of prominent log amplitudes of the target state.
    # i.e. if the state was |psi> = 1/sqrt(6)|0> + sqrt(5/6)|1>, the error on the |1> term would produce a higher (worse) loss function than the same error on the |0> term. In this setup, by a factor of 5
    if weight_psi_sq:
        weightings = abs(jnp.exp(log_amps[indices])) ** 2
    # Else, the weightings are all one and have no effect
    else:
        weightings = jnp.ones(len(indices))
    # Return the loss function defined as sum_{indices sampled}(weighting(index) * |psi_(qGPS)(index) - psi_(exact)(index)|^2)
    return jnp.sum(
        weightings * jnp.abs(sampled_log_amps - (log_amps[indices])) ** 2
    )

def lasso_linear_sweeping(iterations: int, indices: list, configs: list, amps:list, log_amps: list, alpha: float, vs, ha, weighted_according_to_psi_squared):
    """Lasso linear sweeping model for qGPS model to the ground state computed at the top of this code. 
    THIS MUST BE USED IN CONJUCTION WITH initialise_system AND generate_test_data TO MAKE SURE YOURE FITTING THE CORRECT DATA TO THE CORRECT DATA"""
    epsilon = np.array(vs.parameters["epsilon"])  # reset the epsilon tensor
    learning = QGPSLogSpaceFit(
        epsilon
    )  # The way of interfacing the learning model with the qGPS state

    m_sq_error_full_list = []

    #Define LASSO Model
    model = Lasso(alpha=alpha, fit_intercept=False) #alpha is the 'lambda' parameter, defines the L1 penalty the model uses. REGULARISATION

    #Adjust the log_amps data set

    for i in range(iterations):

        if i == 0:
            model.alpha = 0.0
        else:
            model.alpha = alpha
        
        for site in np.arange(ha.hilbert.size):  # For each single site perform an optimisation cycle
            
            learning.reset()
            
            learning.ref_sites = site

            #Setting the feature vector to either be the kernel function generated by the 'set_kernel_mat' method, or the kernel function wieghted by 
            #the square amplitude of the exact fit data. Determined by the 'weighted_according_to_psi_squared' flag.

            prior_mean = 1.0 if site != 0 else 0.0

            #target data and feature vector both individually scaled by |psi|
            if weighted_according_to_psi_squared:
                weightings = jnp.expand_dims(jnp.abs(amps), -1)
                K=learning.set_kernel_mat(update_K=True, confs=configs[indices]) #sampled amplitudes converted to configs as demanded by the 'set_kernel_mat' method
                
                feature_vector = weightings[indices]*K
                
                temp_log_amps = log_amps
                temp_log_amps = temp_log_amps/jnp.std(temp_log_amps)
                temp_log_amps = temp_log_amps - jnp.mean(temp_log_amps)

                fit_data = weightings[indices].flatten()*(temp_log_amps[indices] - prior_mean*np.sum(feature_vector, axis=1))
            else:
                K=learning.set_kernel_mat(update_K=True, confs=configs[indices]) #sampled amplitudes converted to configs as demanded by the 'set_kernel_mat' method
                feature_vector = K
                fit_data = log_amps[indices] - prior_mean* np.sum(feature_vector, axis=1)
                

            #Fitting the model (Computes the optimal weights 'w' that fits the feature vector 'K' to the fit data)
            optimal_weights = model.fit(X=feature_vector, y=fit_data).coef_

            #revert the prior mean adjustment above
            learning.weights = optimal_weights + prior_mean

            #Update the weights and the epsilon tensor held in the learning object.

            learning.valid_kern = abs(np.diag(K.conj().T.dot(K))) > learning.kern_cutoff

            learning.update_epsilon_with_weights()
        
        # Convert the learnt epsilon tensor into log wavefunction amplitudes.
        estimated_log_amps = vs._apply_fun({"params": {"epsilon": learning.epsilon}}, configs)

        # Calculate Error DOING THIS IN THE LOOP IS TEMPORARY UNTIL I KNOW IT WORKS, THEN CAN JUST CALL LOSS FUNCTION ON THE OUTPUTED LOG AMPS
        m_sq_error_full = lossfun(log_amps, configs, learning.epsilon, jnp.atleast_1d(jnp.arange(ha.hilbert.size)), vs, weighted_according_to_psi_squared)
        m_sq_error_full_list.append(m_sq_error_full)

    return estimated_log_amps,m_sq_error_full_list

def overlap_error(configs, amps, log_amps, iterations, indices_to_fit, alpha, vs, ha, weighted_bool):
    """Calculates the overlap of the """
    estimated_log_amps, _ = lasso_linear_sweeping(iterations, indices_to_fit, configs, amps, log_amps, alpha, vs, ha, weighted_bool)
    estimated_amps  = jnp.exp(estimated_log_amps)/jnp.linalg.norm(jnp.exp(estimated_log_amps))
    overlap = abs(estimated_amps.T.dot(amps))
    return overlap


#---------------------------------------------------Plotting, Testing etc etc-------------------------------------------------------------------------------

def generate_meeting_plots_25_02(L,M,alpha,iters,indices_sets,n_plots=1,extra_string=""):
    vs, ha = initialise_system(L=L,M=M)
    e, configs, amps, log_amps = generate_test_data(ha)
    x_ticks = [indices_sets[i] for i in range(len(indices_sets))]
    weight = [True, False]
    for i in weight:
        overlaps = []
        errors = []
        for set in indices_sets:
            overlaps.append(overlap_error(configs, amps, log_amps, iters, jnp.atleast_1d(jnp.arange(set)), alpha, vs, ha, weight[i]))
            _, error = lasso_linear_sweeping(iters, jnp.atleast_1d(jnp.arange(set)), configs, amps, log_amps, alpha, vs, ha, weight[i])
            errors.append(error[-1])
            
        #Plotting Overlaps
        plt.plot(x_ticks, overlaps)
        plt.title("<qGPS|gs> of LASSO Estimator with size of training set, (alpha=" + str(alpha) + ", iters=" +str(iters) +", M="+str(M)+", weights="+str(weight[i])+")", fontsize = 8)
        plt.xlabel("Size of Training Data")
        plt.xticks(x_ticks)
        plt.ylabel("Overlap, <qGPS|gs>")
        plt.savefig("Overlaps: Weights="+str(weight[i])+str(extra_string)+".png")  
        plt.clf()

        #Plotting Errors
        plt.plot(x_ticks, errors)
        plt.title("LSE of LASSO Estimator with size of training set, (alpha=" + str(alpha) + ", iters=" +str(iters) +", M="+str(M)+", weights="+str(weight[i])+")", fontsize = 8)
        plt.xlabel("Size of Training Data")
        plt.xticks(x_ticks)
        plt.ylabel("LSE")
        plt.savefig("Errors: Weights="+str(weight[i])+str(extra_string)+".png")
        plt.clf()

    return None

def plot_overlaps(L,M,alpha,iters,indices_sets):
    vs, ha = initialise_system(L,M)
    _, configs, amps, log_amps = generate_test_data(ha)
    overlaps = []
    for set in indices_sets:
        overlaps.append(overlap_error(
        configs=configs,
        amps = amps, 
        log_amps = log_amps, 
        iterations = iters, 
        indices_to_fit = jnp.atleast_1d(jnp.arange(set)), 
        alpha = alpha, 
        vs = vs, 
        ha = ha, 
        weighted_bool=True
        ))
    plt.plot(indices_sets, overlaps)
    plt.title("<qGPS|gs> of LASSO Estimator vs. size of training set, (alpha=" + str(alpha) + ", iters=" +str(iters) +", M="+str(M)+", weights=False)", fontsize = 8)
    plt.xlabel("Size of Training Data")
    plt.ylabel("Overlap, <qGPS|gs>")
    plt.savefig("Overlaps: Step Size = 10, Weights = True.png")

    return overlaps

def plot_errors(L,M,alpha,iters,indices_sets):
    vs, ha = initialise_system(L,M)
    _, configs, amps, log_amps = generate_test_data(ha)
    errors = []
    for set in indices_sets:
        _, errors_full = lasso_linear_sweeping(iterations = iters, indices = jnp.atleast_1d(jnp.arange(set)), configs = configs, amps = amps, log_amps = log_amps, alpha = 0.001, vs = vs, ha = ha, weighted_according_to_psi_squared = True)
        errors.append(errors_full[-1])
    plt.plot(indices_sets, errors)
    plt.title("LSE of LASSO Estimator vs. size of training set, (alpha=" + str(alpha) + ", iters=" +str(iters) +", M="+str(M)+", weights=False)", fontsize = 8)
    plt.xlabel("Size of Training Data")
    plt.ylabel("LSE of log amplitudes")
    plt.savefig("Errors: Step Size = 10, Weights = True.png")

    return errors

#errors_1 = plot_errors(10,15,0.001,50,[160,170,180,190,200,210,220,230,240,252])
#print(errors_1)
#[100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,252]
#overlaps_1 = plot_overlaps(10,15,0.001,50,[100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,252])
#print(overlaps_1)


#Smaller Tests:

"""vs, ha = initialise_system(L=10,M=15)

e, configs, amps, log_amps = generate_test_data(ha)"""
#print(log_amps)
#print(jnp.linalg.norm(log_amps))

"""estimated_log_amps, full_error_list = lasso_linear_sweeping(
    iterations=50, 
    indices = jnp.atleast_1d(jnp.arange(252)), 
    configs=configs,
    amps = amps, 
    log_amps = log_amps,
    alpha = 0.001,
    vs=vs,
    ha=ha,
    weighted_according_to_psi_squared=True
    )
print(jnp.linalg.norm(log_amps))
print(jnp.linalg.norm(estimated_log_amps))"""

#print(full_error_list)
#print(jnp.exp(estimated_log_amps))
#print(jnp.linalg.norm(jnp.exp(estimated_log_amps)))

"""overlap = overlap_error(
    configs=configs,
    amps = amps, 
    log_amps = log_amps, 
    iterations = 10, 
    indices_to_fit = jnp.atleast_1d(jnp.arange(200)), 
    alpha = 0.0, 
    vs = vs, 
    ha = ha, 
    weighted_bool=False
    )"""

#print(overlap) 

#a=0
#0.9597 with weighted_bool = False
#0.00048 with weighted_bool = True

#a=0.0001

#Keep testing the weighted_bool= False for now. Then fix the True later.


#The Big Test:

#generate_meeting_plots_25_02(L=10, M=10, alpha = 0, iters = 10, indices_sets= [100,120,140,160,180,200,220,240,252])